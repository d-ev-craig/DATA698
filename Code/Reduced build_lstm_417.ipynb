{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Used in LTSMModel Class Instantiation\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConstantMinMaxScaler(series,min_gold,max_gold):\n",
    "    dataset_scaled = (series - min_gold)/(max_gold - min_gold)\n",
    "    return dataset_scaled\n",
    "\n",
    "def ConstantUnScaler(series, min_gold, max_gold):\n",
    "    dataset_unscaled = np.round((series * (max_gold - min_gold)) + min_gold)\n",
    "    return dataset_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_prep_one_hero.ipynb\n",
    "\n",
    "#returns df_allhero with dimensions (124, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    # Class to create our dataset\n",
    "    def __init__(self, df):\n",
    "        self.hero_ids = df['hero_id'].values # Declaring hero_id values\n",
    "        self.time_series = [torch.tensor(ts) for ts in df['gold_t']] # Converting the time_series into Tensors\n",
    "        self.max_length = max(len(ts) for ts in self.time_series) # Grabs max length of all the tensors to pad them with 0s later\n",
    "        self.match_ids = df['match_id'] #Storing the match_id in case we want to view this later for more info\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hero_ids) # Convenient length call\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        hero_id = self.hero_ids[idx]\n",
    "        time_series = self.time_series[idx]\n",
    "        match_id = self.match_ids[idx]\n",
    "\n",
    "        scaled_time_series = ConstantMinMaxScaler(time_series, min_gold, max_gold)\n",
    "        \n",
    "        length = len(scaled_time_series)\n",
    "\n",
    "        padded_time_series = torch.zeros(self.max_length)\n",
    "        padded_time_series[:length] = scaled_time_series\n",
    "        #print(f\"Padded Time Series: {padded_time_series}\")\n",
    "\n",
    "        return hero_id, padded_time_series, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ProcessEmbedding(nn.Module):\n",
    "    def __init__(self, df, embedding_dim):\n",
    "        super(ProcessEmbedding, self).__init__() \n",
    "\n",
    "        self.num_processes = len(df['hero_id'].unique()) # declaring number of different categories of time-series for dimensionialty reasons\n",
    "        self.embedding_dim = embedding_dim # passing our embed size to be a class attribute\n",
    "        self.process_embeddings = nn.Embedding(self.num_processes, embedding_dim)\n",
    "\n",
    "        self.hero_id_to_idx = {hero_id: idx for idx, hero_id in enumerate(df['hero_id'].unique())}\n",
    "\n",
    "    def forward(self, hero_ids):\n",
    "        process_ids = [self.hero_id_to_idx[hero_id.item()] for hero_id in hero_ids]\n",
    "        process_ids = torch.tensor(process_ids)\n",
    "        process_embeddings = self.process_embeddings(process_ids)\n",
    "\n",
    "        print(\"Process Embeddings shape:\", process_embeddings.shape)\n",
    "        print(\"Process Embeddings tensor:\", process_embeddings)\n",
    "\n",
    "        return process_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, process_embedding):\n",
    "        super(LSTMModel, self).__init__() # ensures the correcty PyTorch class is also initialized\n",
    "\n",
    "        self.hidden_size = hidden_size #hyper param \n",
    "        self.num_layers = num_layers #hyper param\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) # Actual LSTM creation\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # Linear Model creation\n",
    "        self.process_embedding = process_embedding # Process Embedding\n",
    "\n",
    "\n",
    "    def forward(self, hero_ids, time_series):\n",
    "        batch_size = time_series.size(0) # pulling dims from the tensor\n",
    "        seq_length = time_series.size(1) # pulling dims from the tensor\n",
    "        \n",
    "        # Get process embeddings for hero_ids\n",
    "        process_embeddings = self.process_embedding(hero_ids)\n",
    "\n",
    "        print(\"Process Embeddings shape:\", process_embeddings.shape)\n",
    "        print(\"Time Series shape:\", time_series.shape)\n",
    "        \n",
    "        # Reshape process embeddings to match the input shape of LSTM\n",
    "        process_embeddings = process_embeddings.unsqueeze(1).repeat(1, seq_length, 1)\n",
    "\n",
    "        print(\"Reshaped Process Embeddings shape:\", process_embeddings.shape)\n",
    "\n",
    "        # Unsqueexing to ensure the time_series shape is 3D like our embedding processing is so that no issues are ran into with torch.cat below\n",
    "        time_series = time_series.unsqueeze(-1)\n",
    "\n",
    "        print(\"Time Series shape with extra dimension:\", time_series.shape)\n",
    "        \n",
    "        # Concatenate process embeddings with time series data\n",
    "        # dim = -1, signifies concatenation across the last dimension (the feature dimension)\n",
    "        input_data = torch.cat((process_embeddings, time_series), dim=-1)\n",
    "        \n",
    "        # Pack the padded sequences\n",
    "        # Packing the padded Sequences is a way of optimizing computation times. We have padded the time series to all be the same length, even though some are only 20 or less\n",
    "        # The packing indicates which are the real values in the time series so that the computation is only ran on those time steps. Details on how are unknown to me thus far.\n",
    "        packed_input = pack_padded_sequence(input_data, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        \n",
    "\n",
    "        packed_output, _ = self.lstm(packed_input, (h0, c0))\n",
    "\n",
    "        # Unpack the output\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # Take the last output of the LSTM\n",
    "        out = self.fc(output[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating Classes and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_embedding = ProcessEmbedding(df_allhero, embedding_dim=84) # we create the embedding vector on unsplit data to ensure all unique hero id's are contained\n",
    "\n",
    "#input_size = process_embedding.embedding_dim + time_series.shape[-1]  # Number of features (embedding_dim + time_series_dim)\n",
    "input_size = process_embedding.embedding_dim + 1 #84 + 1\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1  # Assuming you want to predict a single value\n",
    "\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, process_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = .30\n",
    "\n",
    "train_df, test_df = train_test_split(df_full, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(train_df)\n",
    "test_dataset = TimeSeriesDataset(test_df)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for hero_ids, time_series, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(hero_ids, time_series)\n",
    "        targets = time_series[:, -1]  # Assuming you want to predict the last value of each time series\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * time_series.size(0)\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for hero_ids, time_series, lengths in test_loader:\n",
    "            # Forward pass\n",
    "            \n",
    "            outputs = model(hero_ids, time_series)\n",
    "            targets = time_series[:, -1]  # Assuming you want to predict the last value of each time series\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "\n",
    "            test_loss += loss.item() * time_series.size(0)\n",
    "\n",
    "    test_loss /= len(test_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
