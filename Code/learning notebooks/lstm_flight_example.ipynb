{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_air = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv')\n",
    "timeseries = df_air[[\"Passengers\"]].values.astype('float32')\n",
    "\n",
    "# train-test split for time series\n",
    "train_size = int(len(timeseries) * 0.67)\n",
    "test_size = len(timeseries) - train_size\n",
    "train, test = timeseries[:train_size], timeseries[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-lookback): # \n",
    "        feature = dataset[i:i+lookback]\n",
    "        target = dataset[i+1:i+lookback+1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "         # Normalizing Data\n",
    "    X_min = X.min(dim=1, keepdim=True)[0]\n",
    "    X_max = X.max(dim=1, keepdim=True)[0]\n",
    "    X = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "    Y_min = y.min(dim=1, keepdim=True)[0]\n",
    "    Y_max = y.max(dim=1, keepdim=True)[0]\n",
    "    y = (y - Y_min) / (Y_max - Y_min) \n",
    "    \n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using create_dataset to create our tensors of shape (window size, time steps, features).\n",
    "\n",
    "Window Size: the full length of the time series\n",
    "Time Steps: the number of steps the model can \"lookback\" on\n",
    "Features: Number of features, ours will only be the time series\n",
    "\n",
    "Note that we are returning the train and the test time series in the same size. This is because when we run the model on the test, it is going to use the same number of time steps (lookback = 4) to predict the next value, just like how the model trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 4\n",
    "X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "X_test, y_test = create_dataset(test, lookback=lookback)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of nn.LSTM() is a tuple. The first element is the generated hidden states, one for each time step of the input. The second element is the LSTM cellâ€™s memory and hidden states, which is not used here.\n",
    "\n",
    "The LSTM layer is created with option batch_first=True because the tensors you prepared is in the dimension of (window sample, time steps, features) and where a batch is created by sampling on the first dimension. If batch_first=False, then the shape of the tensor changes. I believe the 2nd or the first changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(50, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "model = AirModel()\n",
    "optimizer = optim.Adam(model.parameters() ,lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight Data Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    if epoch % 100 != 0:\n",
    "        continue\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "        y_pred = model(X_test)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
