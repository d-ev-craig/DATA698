{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dx-wxtGvj5w7","executionInfo":{"status":"ok","timestamp":1714431585638,"user_tz":240,"elapsed":12710,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["import json\n","import pandas as pd\n","from numpy import array\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Used in LTSMModel Class Instantiation\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDlqrOEAkcLO","executionInfo":{"status":"ok","timestamp":1714431606608,"user_tz":240,"elapsed":20985,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}},"outputId":"4597fd38-a34f-4f22-e8a6-ba822f2e7ee1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/698/\")"],"metadata":{"id":"3N75xrN6kc8f","executionInfo":{"status":"ok","timestamp":1714431606817,"user_tz":240,"elapsed":211,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3s39p0k_j5w9","executionInfo":{"status":"ok","timestamp":1714431626113,"user_tz":240,"elapsed":19299,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}},"outputId":"9e84838c-13f5-49dd-c3b5-9376f767e81c"},"outputs":[{"output_type":"stream","name":"stdout","text":["      hero_id gold_t\n","2270       20   None\n","2271      120   None\n","2272       69   None\n","2273       73   None\n","2274       26   None\n","...       ...    ...\n","6915      136   None\n","6916      106   None\n","6917      137   None\n","6918       14   None\n","6919      112   None\n","\n","[76 rows x 2 columns]\n","      hero_id gold_t\n","2270       20   None\n","2271      120   None\n","2272       69   None\n","2273       73   None\n","2274       26   None\n","...       ...    ...\n","6915      136   None\n","6916      106   None\n","6917      137   None\n","6918       14   None\n","6919      112   None\n","\n","[76 rows x 2 columns]\n","Found 194 tensors with length 0 at indices: [230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 2460, 2501, 2512, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 8094, 8095, 8096, 8097, 8098, 8099, 8100, 8101, 8102, 8103, 9524, 9525, 9526, 9527, 9528, 9529, 9530, 9531, 9532, 9533]\n","Found 194 tensors with length 0 at indices: [230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 2460, 2501, 2512, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 8094, 8095, 8096, 8097, 8098, 8099, 8100, 8101, 8102, 8103, 9524, 9525, 9526, 9527, 9528, 9529, 9530, 9531, 9532, 9533]\n","6924\n","2976\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["%run data_prep_one_hero_10step_5horizon.ipynb\n","\n","#torch.load(df_allhero, 'df_allhero.pt')\n","#torch.load(hero_ids, 'hero_ids.pt')\n","#torch.load(df_all_remain, 'df_all_remain.pt')\n","\n","#torch.load(df2_allhero, 'df2_allhero.pt')\n","#torch.load(hero_ids2, 'hero_ids2.pt')\n","#torch.load(df2_all_remain, 'df2_all_remain.pt')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"865_J5kUj5w9","executionInfo":{"status":"ok","timestamp":1714431626114,"user_tz":240,"elapsed":24,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["#df_allhero[['gold_t']]\n","\n","#df_allhero[['gold_t']] ="]},{"cell_type":"code","source":["df2_allhero_avglen"],"metadata":{"id":"xAkelUZRXnsG","executionInfo":{"status":"ok","timestamp":1714431626114,"user_tz":240,"elapsed":23,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}},"outputId":"673256aa-d937-45b8-9002-01c396f8e1e8","colab":{"base_uri":"https://localhost:8080/","height":423}},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       match_id hero_id                                             gold_t  \\\n","0    7530463515       1  [0, 207, 640, 1050, 1497, 1987, 2416, 2872, 32...   \n","1    7538340310       2  [0, 281, 544, 944, 1270, 1699, 2252, 2777, 333...   \n","2    7569905905       3  [0, 130, 257, 367, 610, 700, 889, 979, 1159, 1...   \n","3    7584141103       4  [0, 257, 602, 860, 1385, 1747, 2021, 2277, 248...   \n","4    7538635507       5  [0, 221, 350, 487, 617, 770, 905, 1107, 1197, ...   \n","..          ...     ...                                                ...   \n","119  7538558409     129  [0, 470, 630, 1001, 1295, 1421, 2137, 2709, 31...   \n","120  7553049303     135  [0, 256, 570, 942, 1279, 1600, 1982, 2773, 328...   \n","121  7588454903     136  [0, 537, 1003, 1291, 1646, 1947, 2242, 2724, 3...   \n","122  7525933503     137  [170, 478, 709, 1051, 1228, 1690, 1780, 2371, ...   \n","123  7564044018     138  [0, 209, 514, 621, 866, 1004, 1193, 1351, 1531...   \n","\n","    lane lane_role             radiant_team                dire_team  \\\n","0    3.0       1.0    [7, 74, 114, 104, 84]       [96, 53, 1, 19, 5]   \n","1    1.0       3.0      [20, 7, 51, 41, 55]     [114, 9, 11, 2, 105]   \n","2    1.0       3.0     [53, 26, 48, 76, 33]    [112, 41, 98, 3, 100]   \n","3    1.0       1.0      [23, 4, 64, 49, 87]  [70, 111, 104, 71, 112]   \n","4    3.0       3.0     [5, 69, 25, 84, 120]     [21, 88, 66, 55, 54]   \n","..   ...       ...                      ...                      ...   \n","119  1.0       3.0    [138, 29, 64, 19, 45]    [74, 103, 6, 129, 53]   \n","120  2.0       2.0    [30, 47, 48, 119, 21]     [14, 71, 6, 135, 26]   \n","121  1.0       3.0    [80, 112, 69, 123, 1]    [94, 91, 90, 136, 58]   \n","122  3.0       3.0  [26, 65, 109, 121, 137]    [20, 21, 39, 69, 119]   \n","123  3.0       3.0  [138, 48, 136, 104, 74]    [106, 40, 93, 26, 69]   \n","\n","    radiant_win  \n","0          True  \n","1          True  \n","2         False  \n","3         False  \n","4         False  \n","..          ...  \n","119        True  \n","120       False  \n","121       False  \n","122       False  \n","123        True  \n","\n","[124 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-49048401-e206-4128-844a-c8e7bd165729\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>match_id</th>\n","      <th>hero_id</th>\n","      <th>gold_t</th>\n","      <th>lane</th>\n","      <th>lane_role</th>\n","      <th>radiant_team</th>\n","      <th>dire_team</th>\n","      <th>radiant_win</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7530463515</td>\n","      <td>1</td>\n","      <td>[0, 207, 640, 1050, 1497, 1987, 2416, 2872, 32...</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>[7, 74, 114, 104, 84]</td>\n","      <td>[96, 53, 1, 19, 5]</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7538340310</td>\n","      <td>2</td>\n","      <td>[0, 281, 544, 944, 1270, 1699, 2252, 2777, 333...</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>[20, 7, 51, 41, 55]</td>\n","      <td>[114, 9, 11, 2, 105]</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7569905905</td>\n","      <td>3</td>\n","      <td>[0, 130, 257, 367, 610, 700, 889, 979, 1159, 1...</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>[53, 26, 48, 76, 33]</td>\n","      <td>[112, 41, 98, 3, 100]</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7584141103</td>\n","      <td>4</td>\n","      <td>[0, 257, 602, 860, 1385, 1747, 2021, 2277, 248...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>[23, 4, 64, 49, 87]</td>\n","      <td>[70, 111, 104, 71, 112]</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7538635507</td>\n","      <td>5</td>\n","      <td>[0, 221, 350, 487, 617, 770, 905, 1107, 1197, ...</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>[5, 69, 25, 84, 120]</td>\n","      <td>[21, 88, 66, 55, 54]</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>119</th>\n","      <td>7538558409</td>\n","      <td>129</td>\n","      <td>[0, 470, 630, 1001, 1295, 1421, 2137, 2709, 31...</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>[138, 29, 64, 19, 45]</td>\n","      <td>[74, 103, 6, 129, 53]</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>120</th>\n","      <td>7553049303</td>\n","      <td>135</td>\n","      <td>[0, 256, 570, 942, 1279, 1600, 1982, 2773, 328...</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>[30, 47, 48, 119, 21]</td>\n","      <td>[14, 71, 6, 135, 26]</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>121</th>\n","      <td>7588454903</td>\n","      <td>136</td>\n","      <td>[0, 537, 1003, 1291, 1646, 1947, 2242, 2724, 3...</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>[80, 112, 69, 123, 1]</td>\n","      <td>[94, 91, 90, 136, 58]</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>122</th>\n","      <td>7525933503</td>\n","      <td>137</td>\n","      <td>[170, 478, 709, 1051, 1228, 1690, 1780, 2371, ...</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>[26, 65, 109, 121, 137]</td>\n","      <td>[20, 21, 39, 69, 119]</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>123</th>\n","      <td>7564044018</td>\n","      <td>138</td>\n","      <td>[0, 209, 514, 621, 866, 1004, 1193, 1351, 1531...</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>[138, 48, 136, 104, 74]</td>\n","      <td>[106, 40, 93, 26, 69]</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>124 rows Ã— 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49048401-e206-4128-844a-c8e7bd165729')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-49048401-e206-4128-844a-c8e7bd165729 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-49048401-e206-4128-844a-c8e7bd165729');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7a5ff122-0b0b-42f2-a341-3d57650424dd\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a5ff122-0b0b-42f2-a341-3d57650424dd')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7a5ff122-0b0b-42f2-a341-3d57650424dd button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_db3ba33e-3e90-4cee-8ea8-95a23ce7e59d\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df2_allhero_avglen')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_db3ba33e-3e90-4cee-8ea8-95a23ce7e59d button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df2_allhero_avglen');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df2_allhero_avglen","summary":"{\n  \"name\": \"df2_allhero_avglen\",\n  \"rows\": 124,\n  \"fields\": [\n    {\n      \"column\": \"match_id\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"7519175015\",\n        \"max\": \"7632217207\",\n        \"num_unique_values\": 87,\n        \"samples\": [\n          \"7548914808\",\n          \"7530463515\",\n          \"7553049303\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hero_id\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1\",\n        \"max\": \"138\",\n        \"num_unique_values\": 124,\n        \"samples\": [\n          \"19\",\n          \"44\",\n          \"38\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold_t\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lane\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 1.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.0,\n          1.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lane_role\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 1.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          3.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"radiant_team\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dire_team\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"radiant_win\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"False\",\n          \"True\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iWSC-SpZj5w-","executionInfo":{"status":"ok","timestamp":1714431626114,"user_tz":240,"elapsed":5,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["#len(df_allhero.iloc[0]['gold_t'])"]},{"cell_type":"markdown","metadata":{"id":"S2LbZpekj5w-"},"source":["### TimeSeriesDataset Class"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"_LyuKx4qj5w_","executionInfo":{"status":"ok","timestamp":1714431626114,"user_tz":240,"elapsed":5,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["#type(df_allhero[['gold_t']])"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"aBx7sVa5j5w_","executionInfo":{"status":"ok","timestamp":1714431626114,"user_tz":240,"elapsed":5,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","\n","class TimeSeriesDataset(Dataset):\n","    # Class to create our dataset\n","    def __init__(self, df, lookback):\n","        self.hero_ids = df['hero_id'].values # Declaring hero_id values\n","        self.time_series = df[['gold_t']]\n","        #[torch.tensor(ts) for ts in df['gold_t']] # Converting the time_series into Tensors\n","        self.max_length = max(len(ts) for ts in self.time_series) # Grabs max length of all the tensors to pad them with 0s later\n","        self.match_ids = df['match_id'] #Storing the match_id in case we want to view this later for more info\n","        self.lookback = lookback\n","\n","\n","    def __len__(self):\n","        return len(self.hero_ids) # Convenient length call\n","\n","\n","    def create_windows(self, timeseries, horizon):\n","        X, y = [], []\n","        for i in range(len(timeseries) - self.lookback - horizon + 1):\n","            feature = timeseries[i:i+self.lookback]\n","            target = timeseries[i:i+self.lookback+horizon] # we want the target to contain the timesteps from the lookback AND the steps forward so that the lookback steps will be treated as features\n","            X.append(feature)\n","            y.append(target)\n","\n","\n","        #print(\"Create Window X Obj:\",X)\n","        #print(\"Create Window y Obj:\",y)\n","        #print(\"Create Window Type X:\", type(X))\n","        #print(\"Create Window Type y:\", type(y))\n","        #print(len(X))\n","        #print(len(y))\n","\n","        X = torch.tensor(X)\n","        y = torch.tensor(y)\n","\n","        return X, y\n","\n","\n","\n","    def __getitem__(self, idx):\n","        #print(\"1st Step __getitem__ State: \", idx)\n","        hero_id = self.hero_ids[idx]\n","\n","        time_series = np.array(self.time_series.iloc[idx][0]).astype('float32') # Since the df_allhero 'gold_t' column is a column of lists, we take the\n","        #first row of the df_allhero with .iloc[idx]\n","        # then we access the the first element of the row, which is the list, with [0]\n","        # we convert it to a numpy array, and then convert values to float32\n","        # we do this to be compatible with the ConstantMinMaxScaler()\n","\n","        match_id = self.match_ids[idx]\n","\n","\n","\n","        scaled_time_series = ConstantMinMaxScaler(time_series, min_gold, max_gold)\n","        #print(\"Type of scaled_time_series:\",type(scaled_time_series))\n","        length = len(scaled_time_series)\n","\n","        X, y = self.create_windows(scaled_time_series, horizon = 5)\n","        #print(X.shape, y.shape)\n","        # print(\"Post create_window:\",idx)\n","        # print(\"Type of X\", type(X))\n","        # print(\"Type of y\", type(y))\n","\n","\n","\n","        return hero_id, X, y\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0ev0hr6fj5xA"},"source":["### Embedding Layer"]},{"cell_type":"markdown","metadata":{"id":"TdeRXDpij5xA"},"source":["Embedding module expects the input tensor to have a shape of (batch_size, sequence_length), where batch_size is the number of samples in a batch and sequence_length is the length of each input sequence"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"J33_Gl-uj5xA","executionInfo":{"status":"ok","timestamp":1714431626283,"user_tz":240,"elapsed":7,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["class ProcessEmbedding(nn.Module):\n","    def __init__(self, df, embedding_dim, lookback):\n","        super(ProcessEmbedding, self).__init__()\n","\n","        self.num_processes = len(df['hero_id'].unique()) # declaring number of different categories of time-series for dimensionialty reasons\n","        self.embedding_dim = embedding_dim # passing our embed size to be a class attribute\n","        self.process_embeddings = nn.Embedding(self.num_processes, embedding_dim)\n","\n","        self.hero_id_to_idx = {hero_id: idx for idx, hero_id in enumerate(df['hero_id'].unique())}\n","\n","\n","\n","    def forward(self, hero_id):\n","        process_ids = self.hero_id_to_idx[hero_id]\n","        process_ids = torch.tensor([process_ids])\n","        process_embeddings = self.process_embeddings(process_ids) #.unsqueeze(1).repeat(1, self.lookback, 1)\n","\n","        # print(\"Process Embeddings shape:\", process_embeddings.shape)\n","        # print(\"Process Embeddings tensor:\", process_embeddings)\n","\n","        return process_embeddings\n"]},{"cell_type":"markdown","metadata":{"id":"oZN2kCEoj5xA"},"source":["### LSTM"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"8tNlcgw0j5xA","executionInfo":{"status":"ok","timestamp":1714431626283,"user_tz":240,"elapsed":6,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size, process_embedding):\n","        super(LSTMModel, self).__init__() # ensures the correcty PyTorch class is also initialized\n","\n","        self.hidden_size = hidden_size #hyper param\n","        self.num_layers = num_layers #hyper param\n","\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) # Actual LSTM creation\n","        self.fc = nn.Linear(hidden_size, output_size) # Linear Model creation\n","        self.process_embedding = process_embedding # Process Embedding\n","\n","\n","    def forward(self, batch):\n","        #print(\"LSTM Forward Method Batch Type: \",type(batch))\n","        # Since our Dataset class returns 3 objects, hero_id, X, y and the forward method only expects two, we have to\n","        #    tell our forward method to expect one object and unpack it\n","        hero_ids = batch[0][0] # the _ is a placeholder that doesn't save the last object in the batch which is the y tensor\n","        X = batch[1][0]\n","        X = X.unsqueeze(-1) # To match LSTM model's desired shape of (batch_size, seq_length, input_size)\n","\n","        # print(\"LSTM Forward Method X Type: \",type(X))\n","        # print(\" LSTM Forward Method - X Shape:\", X.shape)\n","        # print(\"LSTM Forward Method Tensor X\",X)\n","        # print(\"LSTM Forward Method hero_ids Type: \",type(hero_ids))\n","        # print(\"LSTM Forward Method hero_ids:\", hero_ids)\n","\n","        batch_size = X.size(0) # pulling dims from the tensor\n","        seq_length = X.size(1) # pulling dims from the tensor\n","\n","        # print(\"LSTM Forward Method - batch_size\", batch_size)\n","\n","        # Get process embeddings for hero_ids\n","        process_embeddings = self.process_embedding(hero_ids)\n","\n","        # print(\"LSTM Forward Method - Process Embeddings Shape Pre-Repeat:\", process_embeddings.shape)\n","        # print(\"LSTM Forward Method - Process Embeddings:\", process_embeddings)\n","\n","        # Reshape process embeddings to match the input shape of LSTM\n","        # process_embeddings = process_embeddings.unsqueeze(1).repeat(1, seq_length, 1)\n","        process_embeddings = process_embeddings.repeat(batch_size, seq_length, 1) # changing process embedding shape to broadcast across the same number of samples in the X tensor\n","        # we do this to match the dimensions so that torch.cat will work\n","        # print(\"LSTM Forward Method - Process Embeddings Shape Post-Repeat:\", process_embeddings.shape)\n","        # dim = -1, signifies concatenation across the last dimension (the feature dimension)\n","        combined_input = torch.cat((X,process_embeddings),dim=-1) #\n","\n","        # print(\"Concat'd Time-Series + Embedding shape:\", combined_input.shape)\n","\n","        # Unsqueexing to ensure the time_series shape is 3D like our embedding processing is so that no issues are ran into with torch.cat below\n","        #time_series = time_series.unsqueeze(-1)\n","\n","        #print(\"Time Series shape with extra dimension:\", time_series.shape)\n","\n","        # Concatenate process embeddings with time series data\n","\n","\n","\n","        # Pack the padded sequences\n","        # Packing the padded Sequences is a way of optimizing computation times. We have padded the time series to all be the same length, even though some are only 20 or less\n","        # The packing indicates which are the real values in the time series so that the computation is only ran on those time steps. Details on how are unknown to me thus far.\n","        #packed_input = pack_padded_sequence(input_data, lengths, batch_first=True, enforce_sorted=False)\n","\n","\n","        # Initialize hidden state and cell state\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n","\n","\n","        #packed_output, _ = self.lstm(packed_input, (h0, c0))\n","\n","        # Unpack the output\n","        #output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","        # Pass combined input to LSTM layer\n","        output, _ = self.lstm(combined_input)\n","\n","        # Take the last 5 outputs of the LSTM\n","        out = self.fc(output[:, -5, :])\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"upVQnTqej5xB"},"source":["### Instantiating Classes and Parameters"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"xk41iRWNj5xB","executionInfo":{"status":"ok","timestamp":1714431626283,"user_tz":240,"elapsed":6,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["lookback = 10\n","#train_dataset = TimeSeriesDataset(df_allhero, lookback)\n","process_embedding = ProcessEmbedding(df2_allhero, embedding_dim=84, lookback=lookback) # we create the embedding vector on unsplit data to ensure all unique hero id's are contained\n","\n","input_size = process_embedding.embedding_dim + 1 #84 + 1\n","hidden_size = 64\n","num_layers = 2\n","output_size = 5  # Assuming you want to predict 5 values\n","\n","model = LSTMModel(input_size, hidden_size, num_layers, output_size, process_embedding)"]},{"cell_type":"markdown","metadata":{"id":"eFPyBntDj5xB"},"source":["### Train Test Split"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ROjpdMwTj5xB","executionInfo":{"status":"ok","timestamp":1714431626283,"user_tz":240,"elapsed":5,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["# test_size = .30\n","\n","# train_df, test_df = train_test_split(df_full, test_size=test_size, shuffle=False)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1TXvNM6qj5xB"},"source":["#### Dataset and Data Load"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Dp4HT8Ahj5xB","executionInfo":{"status":"ok","timestamp":1714431626283,"user_tz":240,"elapsed":5,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["batch_size = 1\n","\n","train_dataset = TimeSeriesDataset(df2_allhero, lookback=lookback)\n","test_dataset = TimeSeriesDataset(df2_allhero_avglen, lookback=lookback)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n","# train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_dataset), batch_size=batch_size, shuffle=False)\n","# test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_dataset), batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"OL22AP1zj5xB","executionInfo":{"status":"ok","timestamp":1714431626283,"user_tz":240,"elapsed":5,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["#len(df_allhero.iloc[32]['gold_t'])"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"i1Pel3NFj5xB","executionInfo":{"status":"ok","timestamp":1714431626283,"user_tz":240,"elapsed":3,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["# # Iterate over the train_loader until the desired index\n","# for idx, (hero_ids, X, y) in enumerate(train_loader):\n","#     print(idx)\n","#     if idx == 32:\n","#         print('Gottem')\n","#         break"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"bAO_dcHej5xB","executionInfo":{"status":"error","timestamp":1714431656916,"user_tz":240,"elapsed":10032,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}},"outputId":"ccaab894-605e-406f-d3b2-5c732533a58d","colab":{"base_uri":"https://localhost:8080/","height":497}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-274dccb777c7>:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n","  X = torch.tensor(X)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/500], Train Loss: 1.3061, Test Loss: 0.1889\n","Epoch: 1\n","Epoch [2/500], Train Loss: 0.6497, Test Loss: 0.2248\n","Epoch: 2\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-18ce298e709c>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# outputs = model(hero_ids, X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assuming you want to predict the last value of each time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-5a83c4469e27>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Pass combined input to LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Take the last 5 outputs of the LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    879\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Training loop\n","num_epochs = 500\n","\n","\n","# Define loss function and optimizer\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","best_test_loss = float('inf')\n","best_model_state = None\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    train_loss = 0.0\n","    print(f\"Epoch: {epoch}\")\n","\n","    for batch in train_loader: #for hero_ids, X, y in train_loader:\n","\n","        hero_ids, X, y = batch #outputs = model(hero_ids, X)\n","        #print(\"Training Loop X Type\", type(X))\n","        #print(\"Training Loop X shape\", len(X))\n","        #print(\"Training Loop X\", X)\n","\n","        #print(\"Training Loop Heros Type\", type(hero_ids))\n","        #print(\"Training Loop Heros shape\", len(hero_ids))\n","        #print(\"Training Loop Heros\", hero_ids)\n","\n","        X = X[0]\n","        # print(\"Training Loop X Type\", type(X))\n","        #print(\"Training Loop X shape\", X.size())\n","        # print(\"Training Loop X\", X)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(batch)\n","        #print(\"Model Outputs: \", outputs)\n","        #print(\"Model Outputs Size: \", outputs.size())\n","\n","        y = y[0]\n","        # print(\"Training Loop Y Type\", type(y))\n","        #print(\"Training Loop Y shape\", y.size())\n","        #print(\"Training Loop Y\", y)\n","\n","        targets = y[:, -5:]  # Assuming you want to predict the last value of each time series\n","        # Maybe should be 5\n","        #print(\"targets last 5 size: \", targets.size())\n","        #print(\"targets last 5: \", targets)\n","        loss = criterion(outputs.squeeze(), targets.squeeze())\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item() * X.size(0)\n","\n","    train_loss /= len(train_dataset)\n","\n","    model.eval()\n","    test_loss = 0.0\n","\n","    with torch.no_grad():\n","         for batch in test_loader: #for hero_ids, X, y in test_loader:\n","            # Forward pass\n","            hero_ids, X, y = batch\n","            X = X[0]\n","            y = y[0]\n","            outputs = model(batch) # outputs = model(hero_ids, X)\n","            targets = y[:, -5:]  # Assuming you want to predict the last value of each time series\n","            loss = criterion(outputs.squeeze(), targets.squeeze())\n","\n","            test_loss += loss.item() * X.size(0)\n","\n","    test_loss /= len(test_dataset)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n","\n","    if test_loss < best_test_loss:\n","      best_test_loss = test_loss\n","      best_model_state = model.state_dict()\n","\n","torch.save(best_model_state, '10step_5horizon_best_model.pth')"]},{"cell_type":"code","source":["best_model_state = torch.load('10step_5horizon_best_model.pth')\n","print(best_test_loss)\n"],"metadata":{"id":"Be8MiuFYCY0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_windows(timeseries,lookback, horizon):\n","    X, y = [], []\n","    for i in range(len(timeseries) - lookback - horizon + 1):\n","        feature = timeseries[i:i+lookback]\n","        target = timeseries[i:i+lookback+horizon] # we want the target to contain the timesteps from the lookback AND the steps forward so that the lookback steps will be treated as features\n","\n","        X.append(feature)\n","        y.append(target)\n","\n","    X = torch.tensor(X)\n","    y = torch.tensor(y)\n","\n","    return X, y\n","\n"],"metadata":{"id":"t9W9Llj-w8sY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(scaled_time_series)"],"metadata":{"id":"Ss4Rwdd34ria"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming you have already trained the model and have the `model` object available\n","model.load_state_dict(best_model_state)\n","# Select a specific time series from df_all_remain\n","index = 8000  # Choose the index of the desired time series\n","selected_row = df_all_remain.iloc[index]\n","\n","# Extract the necessary information from the selected row\n","hero_id = selected_row['hero_id']\n","time_series = np.array(selected_row['gold_t']).astype('float32')\n","\n","# Scale the time series using the same scaling function used during training\n","scaled_time_series = ConstantMinMaxScaler(time_series, min_gold, max_gold)\n","\n","# Create windows from the scaled time series\n","X, y = create_windows(scaled_time_series, lookback, horizon = 5)\n","\n","\n","# Create a batch with the selected time series\n","batch = ((hero_id,), (X,), (y,))\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Disable gradient computation\n","with torch.no_grad():\n","    # Forward pass\n","    outputs = model(batch)\n","\n","    # Get the predicted values\n","    predicted = outputs.squeeze().numpy()\n","\n","\n","#Unscaling values\n","unscaled_targets = ConstantUnScaler(y[:,-5:], min_gold, max_gold)\n","unscaled_preds = ConstantUnScaler(predicted, min_gold, max_gold)\n","\n","# Print the predicted values\n","print(\"Targets:\")\n","print(y[:,-5:])\n","print(\"Predicted values:\")\n","print(predicted)\n","\n","# Print Unscaled Targets\n","print(\"Unscaled Targets\")\n","print(unscaled_targets)\n","print(\"Unscaled Preds\")\n","print(unscaled_preds)"],"metadata":{"id":"H1tdmZASvsXf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","preds_plot = np.ones_like(time_series) * np.nan\n","preds_plot[lookback:len(unscaled_preds)] = unscaled_preds\n","# plot\n","plt.plot(time_series, label = \"Actual Values\")\n","plt.plot(preds_plot, c='r', label = \"Forecasting Predictions\")\n","#plt.plot(test_plot, c='g', label = \"Test Predictions\")\n","\n","plt.title(f\"HeroID:{hero_id} Game Length: {len(time_series)} Minutes, Lookback Window: {lookback}\")\n","\n","# Add a label to the x-axis\n","plt.xlabel('Timestep (Minute)')\n","\n","# Add a label to the y-axis\n","plt.ylabel('Gold')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"sWXrFIV2ysAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Fx00UJz-ISKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWLdXYfbj5xC"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import mean_squared_error\n","\n","# Create an empty dataframe to store the average RMSE scores for each hero\n","df_results_all = pd.DataFrame(columns=['hero_id', 'avg_rmse'])\n","\n","# Get the unique hero_ids from the dataframe\n","hero_ids = df_all_remain['hero_id'].unique()\n","\n","# Loop over each hero_id\n","for hero_id in hero_ids:\n","    # Filter the dataframe for the current hero_id\n","    hero_df = df_all_remain[df_all_remain['hero_id'] == hero_id]\n","\n","    # Create empty lists to store the unscaled targets and predictions for the current hero\n","    unscaled_targets_list = []\n","    unscaled_preds_list = []\n","\n","    # Loop over each record for the current hero\n","    for index, row in hero_df.iterrows():\n","        # Extract the necessary information from the row\n","        time_series = np.array(row['gold_t']).astype('float32')\n","\n","        # Scale the time series using the same scaling function used during training\n","        scaled_time_series = ConstantMinMaxScaler(time_series, min_gold, max_gold)\n","\n","        # Create windows from the scaled time series\n","        X, y = create_windows(scaled_time_series, lookback)\n","\n","        # Create a batch with the selected time series\n","        batch = ((hero_id,), (X,), (y,))\n","\n","        # Set the model to evaluation mode\n","        model.eval()\n","\n","        # Disable gradient computation\n","        with torch.no_grad():\n","            # Forward pass\n","            outputs = model(batch)\n","\n","            # Get the predicted values\n","            predicted = outputs.squeeze().numpy()\n","\n","        # Unscaling values\n","        unscaled_target = ConstantUnScaler(y[-1], min_gold, max_gold)\n","        unscaled_pred = ConstantUnScaler(predicted[-1], min_gold, max_gold)\n","\n","        # Append the last unscaled target and prediction to the respective lists\n","        unscaled_targets_list.append(unscaled_target)\n","        unscaled_preds_list.append(unscaled_pred)\n","\n","    # Calculate the RMSE for the current hero\n","    rmse = np.sqrt(mean_squared_error(unscaled_targets_list, unscaled_preds_list))\n","\n","    # Create a dataframe for the current hero with RMSE value\n","    df_results_hero = pd.DataFrame({'rmse': [rmse]})\n","\n","    # Save the dataframe with the hero_id in the name\n","    df_results_hero.to_csv(f'df_results_{hero_id}.csv', index=False)\n","\n","    # Append the hero_id and RMSE to the df_results_all dataframe\n","    df_results_all = df_results_all.append({'hero_id': hero_id, 'avg_rmse': rmse}, ignore_index=True)\n","\n","# Save the df_results_all dataframe\n","df_results_all.to_csv('df_results_all.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxuLGo0Fj5xC"},"outputs":[],"source":["#test = pd.DataFrame(df_allhero[['gold_t']].iloc[0])\n","#test"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[{"file_id":"1seaZeyVNbm2kERIon8Re6HiSaBeTq19Q","timestamp":1714430779252}]}},"nbformat":4,"nbformat_minor":0}