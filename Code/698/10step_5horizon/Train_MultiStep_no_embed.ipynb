{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aztHoTE5q-vFrlefMWm84IH1weqaIhpb","timestamp":1714853905880}],"machine_shape":"hm","authorship_tag":"ABX9TyN4GPWCSuWYsnHS9GeNTUIJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dx-wxtGvj5w7","executionInfo":{"status":"ok","timestamp":1714854373045,"user_tz":240,"elapsed":7165,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"outputs":[],"source":["import json\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDlqrOEAkcLO","executionInfo":{"status":"ok","timestamp":1714854403962,"user_tz":240,"elapsed":26135,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}},"outputId":"55780325-3aa0-46a0-c53f-9b49a1ef683a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os"],"metadata":{"id":"3N75xrN6kc8f","executionInfo":{"status":"ok","timestamp":1714854420257,"user_tz":240,"elapsed":123,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/Colab Notebooks/698/10step_5horizon\")\n","%run data_prep_10step_5horizon.ipynb\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/698/10step_5horizon\")\n","%run LSTM_Model_Classes_no_embed.ipynb\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/698/\")\n","%run Useful_Functions.ipynb"],"metadata":{"id":"YglRL8Wd-_0r","executionInfo":{"status":"ok","timestamp":1714871129238,"user_tz":240,"elapsed":289,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["batch_size = 1\n","\n","lookback = 1\n","\n","input_size = 1\n","hidden_size = 64\n","num_layers = 2\n","output_size = 1  # Change per number of steps you want to predict out\n","horizon = 1  # Change per number of steps you want to predict out\n","\n","model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n","\n","train_dataset = TimeSeriesDataset(df2_70, lookback=lookback)\n","test_dataset = TimeSeriesDataset(df2_70_remain, lookback=lookback)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"],"metadata":{"id":"3HWlGmqB-Qp6","executionInfo":{"status":"ok","timestamp":1714873710706,"user_tz":240,"elapsed":204,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["len(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uwe-vReQ6zLN","executionInfo":{"status":"ok","timestamp":1714786223269,"user_tz":240,"elapsed":13,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}},"outputId":"48e70384-bb08-48c1-fa5c-c932df33f933"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2991"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["# No CUDA"],"metadata":{"id":"Rb7wEBBx9Eld"}},{"cell_type":"code","execution_count":52,"metadata":{"id":"G_TAnsNB-E4S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714877002377,"user_tz":240,"elapsed":2498090,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}},"outputId":"41e3b7d4-7b99-4c2f-adc3-8518af013f51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0\n","Epoch [1/500], Train Loss: 0.0004, Test Loss: 0.0003\n","Epoch: 1\n","Epoch [2/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 2\n","Epoch [3/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 3\n","Epoch [4/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 4\n","Epoch [5/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 5\n","Epoch [6/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 6\n","Epoch [7/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 7\n","Epoch [8/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 8\n","Epoch [9/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 9\n","Epoch [10/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 10\n","Epoch [11/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 11\n","Epoch [12/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 12\n","Epoch [13/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 13\n","Epoch [14/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 14\n","Epoch [15/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 15\n","Epoch [16/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 16\n","Epoch [17/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 17\n","Epoch [18/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 18\n","Epoch [19/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 19\n","Epoch [20/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 20\n","Epoch [21/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 21\n","Epoch [22/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 22\n","Epoch [23/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 23\n","Epoch [24/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 24\n","Epoch [25/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 25\n","Epoch [26/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 26\n","Epoch [27/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 27\n","Epoch [28/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 28\n","Epoch [29/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 29\n","Epoch [30/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 30\n","Epoch [31/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 31\n","Epoch [32/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 32\n","Epoch [33/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 33\n","Epoch [34/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 34\n","Epoch [35/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 35\n","Epoch [36/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 36\n","Epoch [37/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 37\n","Epoch [38/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 38\n","Epoch [39/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 39\n","Epoch [40/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 40\n","Epoch [41/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 41\n","Epoch [42/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 42\n","Epoch [43/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 43\n","Epoch [44/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 44\n","Epoch [45/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 45\n","Epoch [46/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 46\n","Epoch [47/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 47\n","Epoch [48/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 48\n","Epoch [49/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 49\n","Epoch [50/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 50\n","Epoch [51/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 51\n","Epoch [52/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 52\n","Epoch [53/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 53\n","Epoch [54/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 54\n","Epoch [55/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 55\n","Epoch [56/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 56\n","Epoch [57/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 57\n","Epoch [58/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 58\n","Epoch [59/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 59\n","Epoch [60/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 60\n","Epoch [61/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 61\n","Epoch [62/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 62\n","Epoch [63/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 63\n","Epoch [64/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 64\n","Epoch [65/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 65\n","Epoch [66/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 66\n","Epoch [67/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Epoch: 67\n","Epoch [68/500], Train Loss: 0.0003, Test Loss: 0.0003\n","Early stopping at epoch 68. Best test loss: 0.0003\n"]}],"source":["# Training loop\n","num_epochs = 500\n","patience = 50  # Number of epochs to wait for improvement\n","\n","# Define loss function and optimizer\n","criterion = nn.MSELoss()\n","#criterion = nn.L1Loss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","best_test_loss = float('inf')\n","best_model_state = None\n","epochs_without_improvement = 0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    train_loss = 0.0\n","    print(f\"Epoch: {epoch}\")\n","\n","    for batch in train_loader: #for hero_ids, X, y in train_loader:\n","\n","        hero_ids, X, y = batch #outputs = model(hero_ids, X)\n","        #print(\"Training Loop X Type\", type(X))\n","        #print(\"Training Loop X shape\", len(X))\n","        #print(\"Training Loop X\", X)\n","\n","        #print(\"Training Loop Heros Type\", type(hero_ids))\n","        #print(\"Training Loop Heros shape\", len(hero_ids))\n","        #print(\"Training Loop Heros\", hero_ids)\n","\n","        X = X[0]\n","        # print(\"Training Loop X Type\", type(X))\n","        #print(\"Training Loop X shape\", X.size())\n","        # print(\"Training Loop X\", X)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(batch)\n","        #print(\"Model Outputs: \", outputs)\n","        #print(\"Model Outputs Size: \", outputs.size())\n","\n","        y = y[0]\n","        # print(\"Training Loop Y Type\", type(y))\n","        #print(\"Training Loop Y shape\", y.size())\n","        #print(\"Training Loop Y\", y)\n","\n","        targets = y[:, -horizon:]  # Assuming you want to predict the last value of each time series\n","\n","        #print(\"targets last size: \", targets.size())\n","        #print(\"targets last : \", targets)\n","        loss = criterion(outputs.squeeze(), targets.squeeze())\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item() * X.size(0)\n","\n","    train_loss /= len(train_dataset)\n","\n","    model.eval()\n","    test_loss = 0.0\n","\n","    with torch.no_grad():\n","         for batch in test_loader: #for hero_ids, X, y in test_loader:\n","            # Forward pass\n","            hero_ids, X, y = batch\n","            X = X[0]\n","            y = y[0]\n","            outputs = model(batch) # outputs = model(hero_ids, X)\n","\n","            targets = y[:, -horizon:]  # Assuming you want to predict the last value of each time series\n","\n","            #print(outputs.shape)\n","            #print(targets.shape)\n","            loss = criterion(outputs.squeeze(), targets.squeeze())\n","\n","            test_loss += loss.item() * X.size(0)\n","            #print(f\"Test Loss for Batch: \", test_loss)\n","\n","    test_loss /= len(test_dataset)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n","\n","    if test_loss < best_test_loss:\n","        best_test_loss = test_loss\n","        best_model_state = model.state_dict()\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","\n","    if epochs_without_improvement >= patience:\n","        print(f\"Early stopping at epoch {epoch+1}. Best test loss: {best_test_loss:.4f}\")\n","        break\n"]},{"cell_type":"code","source":["best_test_loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7duzCpmj2QS2","executionInfo":{"status":"ok","timestamp":1714877012477,"user_tz":240,"elapsed":299,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}},"outputId":"abca6335-e5f2-4ada-9c81-6c258f44f3f5"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.00030753854934971345"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["metrics_df = pd.DataFrame({'Metric': ['Training RMSE', 'Test RMSE'],\n","'Value': [ConstantUnScaler(train_loss, min_gold = min_gold, max_gold=max_gold), ConstantUnScaler(test_loss, min_gold = min_gold, max_gold=max_gold)]})\n","#Save the metrics to a CSV file\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/698/10step_5horizon/models/lstm_no_embed\")\n","metrics_file = \"1step_1horizon_RMSE_1_metrics.csv\"\n","metrics_df.to_csv(metrics_file, index=False)\n","print(f\"Metrics saved to {metrics_file}\\n {metrics_df}\")\n","torch.save(best_model_state, '1step_1horizon_RMSE_1.pth')"],"metadata":{"id":"LQbaIZ53hNQC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714877050435,"user_tz":240,"elapsed":119,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}},"outputId":"76fdfa03-9f52-4809-8844-5e9aca6bc819"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Metrics saved to 1step_1horizon_RMSE_1_metrics.csv\n","           Metric  Value\n","0  Training RMSE   35.0\n","1      Test RMSE   35.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qPTEjpyIgOM5","executionInfo":{"status":"ok","timestamp":1714872507556,"user_tz":240,"elapsed":140,"user":{"displayName":"Daniel Craig","userId":"14982667842642761541"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["10Step_5Horizon_RMSE_1Embed: Train Loss:    Test Loss:"],"metadata":{"id":"2LFvEPCXW7Yb"}}]}